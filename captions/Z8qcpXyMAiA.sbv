0:00:00.560,0:00:06.960
hello streaming data is everywhere and its growth 
seems unstoppable activity from social networks

0:00:06.960,0:00:12.080
website analytics and sensors and iot devices 
generates a constant flow of new data coming at

0:00:12.080,0:00:17.520
us from a huge number of sources in this video 
we'll see how redis can help us capture manage

0:00:17.520,0:00:29.840
and make sense of these large and constantly 
moving volumes of data right out of the box

0:00:31.440,0:00:35.600
streaming data is data that's generated 
continuously often from a large number of

0:00:35.600,0:00:40.320
concurrent sources one useful way of thinking 
about a stream is as a series of events

0:00:40.960,0:00:46.480
every entry in the stream represents a new event 
for example you can imagine recording a stream of

0:00:46.480,0:00:52.160
weather sensor readings at a given location every 
entry in the stream might consist of a temperature

0:00:52.160,0:00:57.920
a humidity measurement and the wind direction 
and speed because streams and events are such

0:00:57.920,0:01:02.800
a useful abstraction and occur so often in the 
real world redis provides a data structure called

0:01:02.800,0:01:08.000
a stream in the rest of this video i'm going to 
introduce some streaming concepts and then i'll

0:01:08.000,0:01:12.720
demonstrate some of the basic commands you can 
use to manipulate streams and redis so let's

0:01:12.720,0:01:18.080
start with concepts in a distributed application 
architecture the components that write to a stream

0:01:18.080,0:01:23.120
are called producers the data they generate 
is added to a stream with each entry having a

0:01:23.120,0:01:29.040
unique identifier and its specific fields at the 
other end of the stream are one or more consumers

0:01:29.600,0:01:34.000
these consumer processes read entries from 
the stream and process them as necessary

0:01:35.280,0:01:38.800
what's interesting is that you can have more 
than one consumer reading from your stream

0:01:39.440,0:01:44.480
each consumer has its own role for example one 
type of consumer might create notifications

0:01:44.480,0:01:49.360
in a mobile application when certain trigger 
values are seen in the data another consumer

0:01:49.360,0:01:54.320
might write all entries to a data warehouse for 
later analysis while a third consumer could act

0:01:54.320,0:01:59.120
as a producer for yet another stream adding 
only a subset of the entries to the new stream

0:02:00.080,0:02:05.600
producers and consumers often operate at different 
rates the stream acts as a buffer between them as

0:02:05.600,0:02:10.640
well as the decoupling mechanism this means 
that producers and consumers don't directly

0:02:10.640,0:02:14.640
communicate with each other so don't need to 
know anything about each other's implementations

0:02:16.160,0:02:20.640
now let's talk about the reddit stream data 
structure itself a reddit stream is a data

0:02:20.640,0:02:25.680
structure that behaves like an appendally log 
once added an entry in a stream is immutable

0:02:26.240,0:02:31.680
each entry in a reddest stream has a unique id 
and by default these ids are timestamp prefixed

0:02:32.320,0:02:36.240
this means that a reddest stream 
keeps entries ordered as a time series

0:02:36.240,0:02:40.800
stream entries look a lot like a redis 
hash each is a set of name value pairs

0:02:41.360,0:02:45.920
it's also worth noting that reader streams are 
schema-less while each stream entry has to have

0:02:45.920,0:02:51.920
at least one name value pair they don't all have 
to use the same structure redis allows consumers

0:02:51.920,0:02:57.280
to read stream entries in order consumers can also 
efficiently seek to any entry within the stream

0:02:58.480,0:03:02.560
so enough theory let's look at redis 
streams in action we'll see how they

0:03:02.560,0:03:07.520
fit into a common use case for streaming data 
systems recording real-time crowdsourced data

0:03:09.040,0:03:13.200
imagine we're building a mobile app that allows 
users to check in at all sorts of businesses

0:03:13.200,0:03:18.880
public spaces and workplaces users provide star 
ratings based on their experience at each location

0:03:19.520,0:03:23.520
each time a user checks in they'll pick out 
their location from a list our app provides

0:03:24.160,0:03:28.640
they'll then select a star rating from 0 to 5 
and the app will send this data along with their

0:03:28.640,0:03:34.800
user id to a server users earn prize draw entries 
for each check-in and we periodically offer cool

0:03:34.800,0:03:40.000
prizes to randomly drawn winners this incentive 
encourages users to check in often improving

0:03:40.000,0:03:46.080
their chance of winning a typical check-in can 
be represented as a set of name-value pairs here

0:03:46.080,0:03:53.600
we have a check in for user id 99 at venue id 
103 the user decided to give this venue 4 stars

0:03:55.120,0:04:00.240
readings from our many users are sent continuously 
to our servers and those servers produce entries

0:04:00.240,0:04:05.520
into our redis stream once this fire hose 
of jumbled data is added to a reddit stream

0:04:05.520,0:04:11.360
we can begin to organize and make sense of 
it let's see how each check-in finds its way

0:04:11.360,0:04:17.920
into a reddit stream first like other redis data 
structures a stream is associated with a redis key

0:04:18.480,0:04:24.160
we'll use the key check-ins to identify the 
incoming stream of check-in data the x-add command

0:04:24.160,0:04:30.800
adds a new entry to a stream here we're adding 
an entry to the check in stream from user 90781

0:04:31.680,0:04:37.520
this user is visiting location 348 and 
rating it three stars the asterisk in the

0:04:37.520,0:04:42.400
exact command tells redis to assign this entry 
a unique id consisting of the current timestamp

0:04:42.400,0:04:48.000
plus a sequence number exside returns the 
id that redis is assigned to the new entry

0:04:48.720,0:04:52.880
the first part is the timestamp in milliseconds 
and the second is a sequence number

0:04:53.760,0:04:58.880
since stream ids must be unique this convention 
ensures that we can add as many entries as we need

0:04:58.880,0:05:04.320
in the same millisecond now that we have check-ins 
flooding into our stream it's time to think about

0:05:04.320,0:05:10.000
how the business can make sense of them as 
redis assigns each entry a timestamp based id

0:05:10.000,0:05:15.440
one way that a consumer can view the stream is as 
a time series here we're using the x range command

0:05:15.440,0:05:20.160
to read entries in the check-in stream that fall 
between the specified start and end time stamps

0:05:21.200,0:05:25.120
x-range returns each entry whose id 
falls within the specified time period

0:05:26.160,0:05:28.880
entries are returned in 
order with the oldest first

0:05:31.520,0:05:37.840
to limit the number of entries returned 
we can use the optional count modifier

0:05:40.080,0:05:44.640
if we want the most recent entries first 
we can use the x rev range command instead

0:05:45.680,0:05:52.000
note that here we specify the time period in the 
reverse order with the end time stamp coming first

0:05:52.000,0:05:55.920
and if we don't know what time period the 
entries in the stream cover we can use both

0:05:55.920,0:06:01.280
x range and x rev range with a special plus 
and minus operators to represent the highest

0:06:01.280,0:06:06.880
and lowest timestamps respectively here we're 
retrieving the oldest two entries in the stream

0:06:09.040,0:06:14.400
but really streams are all about real-time data 
consumption so we want to build consumers that

0:06:14.400,0:06:19.920
continuously receive data we could achieve this 
by pulling the stream using the x-range command

0:06:19.920,0:06:25.200
but that's inefficient ideally we want a 
command that lets us consume the stream

0:06:25.200,0:06:28.640
blocking when we've seen all the 
entries until a producer adds new ones

0:06:29.360,0:06:35.200
this is one of the use cases for the x read 
command xreed can consume one or more streams

0:06:35.200,0:06:40.640
optionally blocking until new entries appear here 
i'm calling x-read against the check-in stream

0:06:41.440,0:06:47.760
x-read consumes the stream returning entries whose 
ids are greater than the one provided here i'm

0:06:47.760,0:06:53.280
asking for all entries with an id greater than 
0 the beginning of the stream and redis returns

0:06:53.280,0:06:58.880
the entire stream we'll need to note the id of 
the latest entry for subsequent calls to x read

0:07:00.000,0:07:05.040
we can also invoke x-read in a way that blocks the 
consumer until new entries are added to the stream

0:07:06.720,0:07:12.080
to use xread in a blocking context i'll 
provide the last entry id for my previous call

0:07:12.080,0:07:16.720
i'll also specify i want to consume a single 
new entry and how long to block in case the

0:07:16.720,0:07:22.720
stream doesn't yet contain any entries newer than 
the one whose id i'm supplying here i'm telling

0:07:22.720,0:07:29.840
x-read to block for up to 5000 milliseconds or 5 
seconds when i run this command it blocks because

0:07:29.840,0:07:34.320
no new messages with higher ids than the one 
i provided have yet been added to the stream

0:07:35.840,0:07:40.560
while the consumer is blocked i'll use a second 
redis cli session to add a new entry to the stream

0:07:41.440,0:07:46.000
as soon as i do that the consumer unblocks 
and xreed returns the newly added entry

0:07:47.120,0:07:50.560
if no new entries have been added 
in a 5 second blocking period

0:07:50.560,0:07:55.200
xreed would have returned a nil response i 
could then choose the block again or give

0:07:55.200,0:08:00.480
up trying and do something else it's important 
to note that consuming the stream using x range

0:08:00.480,0:08:06.960
x rev range and x read doesn't remove entries as 
they are retrieved entries remain in the stream

0:08:06.960,0:08:11.840
allowing other consumers to read the entire 
data set each processing it in its own way

0:08:12.640,0:08:18.080
here one consumer is maintaining running average 
star ratings a second writes the entries to a data

0:08:18.080,0:08:23.040
warehouse and a third adds entries to another 
stream if the star rating is below a triggered

0:08:23.040,0:08:28.720
threshold so if new check-ins are constantly 
pouring into our stream and consumers reading them

0:08:28.720,0:08:33.520
aren't removing them then how do we contain this 
seemingly never-ending growth remember that a

0:08:33.520,0:08:38.960
stream models independently log this means that 
the order and content of entries can't be modified

0:08:38.960,0:08:44.640
once they've been added however redis supports a 
trimming strategy to manage a stream's memory use

0:08:45.200,0:08:49.280
let's assume our stream now contains tens of 
thousands of entries and that we need to control

0:08:49.280,0:08:55.200
its growth trimming a stream removes its oldest 
entries that is those of the lowest timestamp

0:08:55.200,0:09:02.240
ids this frees up memory associated with these 
entries let's see how that works in practice redis

0:09:02.240,0:09:09.280
commands can be used to trim a stream the first of 
these is xtrim this command can be run at any time

0:09:09.280,0:09:14.960
and trims the stream's length to a specified new 
length here we're using xtrim to trim our check in

0:09:14.960,0:09:20.640
stream to a new maximum length of 10 000 entries 
the command returns the number of entries removed

0:09:21.440,0:09:25.840
in this case we removed 20 000 in one 
of the oldest entries from the stream

0:09:25.840,0:09:31.120
leaving the newest ten thousand the second way to 
trim the oldest entries from the stream is to do

0:09:31.120,0:09:37.120
so while adding a new entry with the exact command 
and this is the more common strategy if we know

0:09:37.120,0:09:42.240
that we always want to keep the check in stream to 
a length of 10 000 or less then we simply specify

0:09:42.240,0:09:48.480
that on every call to x ad like you see here 
as you saw earlier xad returns the id of the

0:09:48.480,0:09:54.000
entry that was added but doesn't tell us what the 
new length of the stream is for that we can use

0:09:54.000,0:09:59.760
xlan and we see that the stream has been capped 
to 10 000 entries including the newly added one

0:10:01.440,0:10:05.760
there's a lot more to reddit streams of what 
i've presented here so if you're interested in

0:10:05.760,0:10:10.400
learning more you should check out redis streams 
our free online course from redis university

0:10:11.200,0:10:15.360
i know streams can seem like a fire hose 
of information so thanks for sticking with

0:10:15.360,0:10:30.000
me and forwarding this river together 
happy learning and see you again soon
